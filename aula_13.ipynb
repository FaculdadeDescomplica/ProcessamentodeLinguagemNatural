{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM922B4Cgy2vAVE5na2Ul4C"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Processamento de Linguagem Natural - Faculdade Descomplica\n","# Aula 13 - Pipelines de Linguagem Natural e Processamento em Lote\n","\n","Prof. Elvis de Souza"],"metadata":{"id":"f1eFM4YcGXGg"}},{"cell_type":"markdown","source":["# Carregando bibliotecas"],"metadata":{"id":"ZZ6VtgHpNkDE"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"JjOf_VRZGPTv"},"outputs":[],"source":["! pip3 install spacy\n","! python3 -m spacy download pt_core_news_sm\n","! pip3 install wikipedia"]},{"cell_type":"code","source":["import spacy\n","import wikipedia\n","import json\n","\n","wikipedia.set_lang(\"pt\")\n","nlp = spacy.load(\"pt_core_news_sm\")"],"metadata":{"id":"zAUfTX2FGofi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Carregando texto"],"metadata":{"id":"fRg50wqqNm2N"}},{"cell_type":"code","source":["texto = wikipedia.page(\"PLN\")"],"metadata":{"id":"K2HNxg-GG-Er"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["texto"],"metadata":{"id":"TSCTIGFIH91L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["texto.content"],"metadata":{"id":"ILJzr-q_H-lP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Processando texto com spaCy e explorando funções"],"metadata":{"id":"9At7FFSiNoit"}},{"cell_type":"code","source":["doc = nlp(texto.content)"],"metadata":{"id":"dS435ogsILKn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["type(doc)"],"metadata":{"id":"eI9SHpc4IULg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["doc[0]"],"metadata":{"id":"Mv8H13QKIWPf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(doc)"],"metadata":{"id":"eEJzri4zIZrP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(doc.to_json())"],"metadata":{"id":"F4JppuvAJR6_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(json.dumps(doc.to_json(), indent=4, ensure_ascii=False))"],"metadata":{"id":"M20MYIryaHSM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["doc[0].pos_"],"metadata":{"id":"95albu94Ia6X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["doc[0].dep_"],"metadata":{"id":"nsn5ixAPIc8I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["doc[0].head"],"metadata":{"id":"7u-ZbWq_IfPp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["(doc[0].text, doc[0].pos_ + \"/\" + doc[0].dep_, doc[0].head)"],"metadata":{"id":"G2Szd-anIh2n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Criando funções que irão compor o pipeline"],"metadata":{"id":"DYy8oCRpN3rV"}},{"cell_type":"code","source":["def extrair_sintaxe(doc):\n","    saida = []\n","    for token in doc:\n","        saida.append((token.text, token.pos_ + \"/\" + token.dep_, token.head.text))\n","    return saida"],"metadata":{"id":"aQYE_KuyIyYn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["extrair_sintaxe(doc)"],"metadata":{"id":"KgzF4layL6Rc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def extrair_sujeitos(doc):\n","    saida = []\n","    for token in doc:\n","        if token.dep_ == \"nsubj\":\n","            saida.append((token.text, token.head.text))\n","    return saida"],"metadata":{"id":"7ltA51GaMD78"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["extrair_sujeitos(doc)"],"metadata":{"id":"DnHu-FxjMimU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Definição do pipeline"],"metadata":{"id":"OH69nJR4N6Vh"}},{"cell_type":"code","source":["def pipeline(texto, nlp):\n","    doc = nlp(texto)\n","    saida = doc.to_json()\n","    saida['sintaxe'] = extrair_sintaxe(doc)\n","    saida['sujeitos'] = extrair_sujeitos(doc)\n","    return saida"],"metadata":{"id":"hRsUbQF-MkTU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pipeline(\"Processamento de Linguagem Natural é uma subárea da inteligência artificial.\", nlp)"],"metadata":{"id":"WBseciPNNIor"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Vídeo 4: Desenvolvimento de um pipeline completo de PLN em Python"],"metadata":{"id":"BEiuiLLrODhB"}},{"cell_type":"code","source":["import os\n","import uuid\n","import datetime"],"metadata":{"id":"Cmd4RMquOGol"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Criando pastas e arquivo de log"],"metadata":{"id":"WClhhkCJdfHi"}},{"cell_type":"code","source":["if not os.path.isdir(\"raw_texts\"):\n","    os.mkdir(\"raw_texts\")\n","\n","if not os.path.isdir(\"processed_texts\"):\n","    os.mkdir(\"processed_texts\")\n","\n","log_file = open(\"log.txt\", \"a\")"],"metadata":{"id":"xJMjVxdLTsft"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Extraindo documentos"],"metadata":{"id":"OUwD_pRtdhL7"}},{"cell_type":"code","source":["paginas = []\n","for letra in \"abcdefghijklmnopqrstuvwxyz\":\n","    paginas.extend(wikipedia.search(letra, results=5))"],"metadata":{"id":"dnwmv3MeT6b6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(paginas)"],"metadata":{"id":"b0v6RPNcU_M8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["paginas"],"metadata":{"id":"emW1wBqvVLbw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["uuid.uuid4()"],"metadata":{"id":"yvIU1541ViqD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tqdm\n","# ATENÇÃO: Demora 2 minutos.\n","for pagina in tqdm.tqdm(paginas):\n","    try:\n","        texto = wikipedia.page(pagina).content\n","    except wikipedia.DisambiguationError:\n","        log_file.write(f\"[{str(datetime.datetime.now())}] DisambiguationError: {pagina}\\n\")\n","    except wikipedia.PageError:\n","        log_file.write(f\"[{str(datetime.datetime.now())}] PageError: {pagina}\\n\")\n","    else:\n","        with open(f\"raw_texts/{uuid.uuid4()}.txt\", \"w\") as f:\n","            f.write(texto)\n","        log_file.write(f\"[{str(datetime.datetime.now())}] OK: {pagina}\\n\")"],"metadata":{"id":"0a6LvH6dVN58"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(os.listdir(\"raw_texts\"))"],"metadata":{"id":"FWe3tw9MVmp8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Transformando documentos e armazenando"],"metadata":{"id":"WNgYiiWUdm0V"}},{"cell_type":"code","source":["import json\n","\n","nlp = spacy.load(\"pt_core_news_sm\")\n","\n","for arquivo in tqdm.tqdm(os.listdir(\"raw_texts\")):\n","    with open(\"raw_texts/\" + arquivo) as f:\n","        texto = f.read()\n","\n","    processado = pipeline(texto, nlp)\n","\n","    timestamp = str(datetime.datetime.now())\n","    processado['timestamp'] = timestamp\n","    log_file.write(f\"[{str(datetime.datetime.now())}] Pipeline: {arquivo}\\n\")\n","\n","    with open(\"processed_texts/\" + arquivo + \".json\", \"w\") as f:\n","        json.dump(processado, f)\n","\n","    os.remove(\"raw_texts/\" + arquivo)"],"metadata":{"id":"iJbi7M2CXGJJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["log_file.close()"],"metadata":{"id":"rTK2taSMXrPr"},"execution_count":null,"outputs":[]}]}